1. 아나콘다 가상환경 구축
 - Anaconda navigator 가동
 - Environments > Create > (name)DataCrawling, python 3.6 > 생성
 - cd ~/analysis/crawling/requirements.txt 생성 : 해당 프로젝트에서 사용하는 패키지를 기술
   ------------------------------------------------------------------------
   requests==2.18.4
   beautifulsoup4==4.6.0
   pandas==0.23.0
   numpy==1.14.3
   ------------------------------------------------------------------------
   Environments > DataCrawling > Open Terminal 
   (DataCrawling) $ cd C:\study\git\py_projects\analysis\crawling(프로젝트 내 requirements.txt 위치)
   - 가상환경 내에 설치된 패키지 목록 보기 (단, 약간의 목록 차이는 존재)
   (DataCrawling) $ conda list 
   or 
   (DataCrawling) $ pip list 
   - 프로젝트에 필요한 패키지 설치
   (DataCrawling) $ pip install -r requirements.txt
   or 
   (DataCrawling) $ conda install --file requirements.txt

2. 주피터 구동

3. 데이터 
 - 현재 상황
   1) 빅데이터
      원유, 전통적인 데이터 관리법으로 다루기 힘든 데이터를 다 아우른다
   
   2) 데이터 과학 => 새로운 가치(value) 생성
      정유공장 -> 대량의 데이터를 분석하여, 지식을 추출하는 방법까지 아우른다
   
   3) 구분
      > 빅데이터와 전통적인 데이터와 구분
      - 규모 : 얼마나 많은 데이터가 존재하고, 계속 생성되는가
      - 다양성 : 데이터의 종류가 얼마나 많은가
      - 속도 : 새로운 데이터가 얼마나 빠르게 생성되는가
      - 정확성 : 데이터가 얼마나 정확한가
   
   4) 통계학자, 데이터 과학자의 주된 차이점
      - 빅데이터를 다루는 능력, 하둡(스파크, 클러스터) 참고
      - 머신러닝(딥러닝) : AI>머신러닝>딥러닝
      - 컴퓨팅 능력
      - 알고리즘 구축 능력 (새로 만드는가? 혹은 조합을 만들어내는가?)
      
   5) 활용
      - 산업 : 고객, 업무, 직원, 상품에 대한 '통찰'을 얻기 위해 데이터 과학과 빅데이터를 활용 
      - 기업 : 교차판매, 상향 판매, 개인화 등등 -> 구글 애드샌드 활용 -> 맞춤광고 등
      - 예 
        a) 피플 애널리스트 : 텍스트 마이닝 적용 -> 직원 사기 점검, 직원간 네트워크 연구 등
            -> 머니볼(메이저리그 특정밑 이야기, 데이터 분석을 통해 발전시킨 사례)
        b) 금융기관 : 주식시장 예측, 대출 위험 평가, 신규 고객유치, 퀀트들의 개발 알고리즘을 통한 컴퓨터 자동 거래
        c) 정부기관 : 공공 데이터를 공개, 이를 통한 데이터 통찰을 수행할 수 있는 기회 제공, 어플의 개발 유도
            -> 수백만의 개인 감시용으로 사용 : 빅브라더 -> 이메일, 지도, 검색 등을 통해 수백만의 데이터를 수집하고 예측 
        d) 비정부기관 : NGO -> 자원봉사를 통해서 데이터를 구축 및 분석
        e) 대학 
            -> 연구, 학습 경험 개선, 코세라, 유닷시티, 에덱스 등
            -> 다양한 대학 기관들이 데이터를 제공, 
            -> 코렐, 머신러닝 때 따로 제공
 
 - 데이터의 종류
   1) 구조적 데이터
    - 데이터 모델에 의존적인 구조
    - 레코드 내에 고정된 필드가 존재
    - 데이터베이스의 테이블, 엑셀파일
    - 관련 조회 -> sql(구조화된 질의언어) -> 비구조적 데이터에 비해 사이즈가 작다
    - 정제된 데이터
    
   2) 비구조적 데이터
    - 데이터 모델에 적합하지 않은 데이터, 잘 안 맞는다
    - 이메일(발신자, 제목, 본문은 구조적이나, 본문의 내용은 비구조적이다. 정의할 수 없다)
 
   3) 자연어 데이터
    - 언어학에 대한 지식을 필요로 하고, 처리가 까다롭다
    - 이미 파악된 분야 : 개체 인식, 주제 파악, 요약문 작성, 텍스트 완성, 정서(감정) 분석 등
    - 문제점 : 한 분야에 맞는 모델을 개발하면 다른 분야로 일반화를 할 수 없다
    - 한계 : 텍스트의 행간이라는 분야는 아직 풀어내지 못하는 단계, 같은 단어도 감정이 다르게 표출되기 때문에 어려운 분야
    
   4) 기계 생성 데이터
    - 사람의 개입 없이 자동으로 생성된 정보
    - 주체 : 컴퓨터, 프로세서, 어플리케이션, 장비, 센서
    - 해당 데이터는 주요한 데이터 공급원
    - 산업 데이터의 시장 가치 예측은 2020년 기준 600조원 예측(위키본)
    - IDC 쪽에서는 사물 간 연결에서 발생되는 데이터는 기존의 26배로 예측
    
   5) 그래프 기반 데이터(네트워크 데이터)
    - 수학 장르의 그래프 이론을 기반으로 데이터를 설명
    - 그래프 : 짝을 이루는 객체간의 관계를 모델링하는 수학적인 구조
      -> 객체 간의 관계, 인접에 초점을 맞춘 데이터이다.
    - 구성 : 꼭짓점(node), 변(edge), 속성(property)들을 가지고 데이터를 표현
    - 예시 : 소셜 네트워크를 표현하는 가장 적합, 
             어떤 사람의 영향력을 평가, 
             사람들 사이의 최단 경로를 수치화
             ex) 링크드인 : 누가, 어떤 회사에 다니는가
                 트위터, 페이스북, 인스타그램 : 팔로워
             페이스북의 친구, 링크드인 상의 친구 접점, 넷플릭스에서 영화 취향
     -> 새로운 데이터를 통찰 -> 질의 -> 데이터를 분석/규명/설명
    - 그래픽X 
    
   6) 오디오, 비디오 데이터
    - 가장 까다로운 데이터
    - 2014년 기준 MLB에서 생중계 시, 
      경기 분석을 하기 위해서 영상의 분량을 경기당 7TB로 늘려서 처리, 
      고속 카메라가 공, 선수의 움직임을 다 포착하여 베이스라인과 비교 등을 실시간 계산 처리
    - 딥마인드 : 비디오 게임 플레이 알고리즘 개발 -> 구글 인수 -> 알파고
    
   7) 스트리밍 데이터
    - 어떤 사건이 발생 -> 데이터가 발생 -> 시스템으로 흘러들어간다
    - 트위터의 실시간 트렌드, 운동 경기 생중계 데이터, 공연 데이터, 주식 시장 데이터
 
 
 - 연구 목표 설정
   1) 통계/시각적 분석
     - 그래프데이터를 이용한 요리 추천 분석
     - 부산시 cctv 현황분석
     - 미래에 우리나라의 어느 지역이 인구가 몇년 내에 소멸하여, 이에 따른 정책 변환 추진(붙이기 나름)
     - 대선, 총선 결과 분석
     - 주식 데이터(시계열) 분석을 통한 근거리 주식장 분석
     - 스타트업 기업 통계 분석
     - 음악 데이터를 이용한 년도별 인기 통계를 기반으로 향후 유행장르 예측
     - 항공운항데이터나 고객 데이터를 이용한 마케팅 분석
       -> 특가 분석을 통한 고객 유입
          
   2) 머신러닝/지도학습
     => 학습, 예측
     -> 이미 변수(특성)들과 답이 존재한다 -> 이를 학습시키고
     -> 한번도 접해보지 못한 데이터를 예측했을 때 정확도
     
    - 편지 봉투에 손글씨로 작성한 우편번호 숫자 판별
    - 의료 영상 이미지 기반 종양/암 등 질병 판단
    - 의심스러운 신용카드 거래 감지 -> 이상 징후 감지/탐지
    - 텍스트 마이닝을 기반으로 게시글 등을 텍스트 전체 맥락 분석
    - 영화 평점, 리뷰 등을 기반으로 넷플릭스 영화 추천 시스템의 기존 정확도 1% 향상
    - 업리프트 모델링으로 마케팅의 효율성 증가(역학 통계의 다이렉트 마케팅 기법 -> 무작위 대조시험의 결과를 분석하여 신약이 효과가 있었는가? 광고 메일을 어떤 유형의 고객에게 보내야 효과가 있는가?)
    - 알파벳의 언어빈도의 차이를 기반으로 언어를 판독해내는 기술
    - 게임 접속 로그 및 데이터를 제공하여 고객 이탈을 예측하는 모델
    - 연비 예측
    - 콘크리트 혼합물의 압축 강도 예측
    - 스마트카 배기량 분석에 따른 운전자 연소득 예측
    
   3) 머신러닝/비지도학습
    - 답이 없다
    - 블로그 글의 주제를 구분해 내는 모델
    - 고객 취향 유형에 맞춰서 그룹 분류
    - 비정상적인 웹사이트 탐지
    
   4) 딥러닝
    - 자율주행을 위한 영상 분석
    - RNN을 이용한 번역서비스
    - RNN을 이용한 챗봇
    - CNN과 opencv를 이용한 영상 인식
    - 자율주행 중인 스마트카의 위험 징후 판별
 
 - 데이터과학 진행과정
    1) 연구목표 설정
     - 무엇을 조사?
     - 그 분석 결과로 회사/공공에서는 어떤 이익을 도출할 것인가?
     - 어떤 데이터와 자원이 필요한가? (일정, 업무분장 등)
     - 가장 중요한 것은 최종 결과물에 대한 검토 -> 시작점
     - 이런 결과물은 의사결정에 재료가 될 수도, 업무의 한 파트가 될 수도 있다
    
    2) 데이터 획득
     2-1. level 1
      - 제공이 된다
      - 사내 데이터, 공공 데이터, 대학 및 연구기관의 제공 데이터
      - 콘테스트 데이터(국내대회, 해외대회(kaggle))
      -> But, 상업성이 없고, 대부분 정제된 데이터 
     
     2-2. level 2
      - open API 사용
      - http 통신을 통해서 응답 데이터를 통해 수집
        ex) kakao, naver 등 포털이나 대기업이 제공하는 open API를 활용
      -> But, 쿼리 제한(일일 쿼리 수), 정제된 데이터
     
     2-3. level 3
      - web scraping(웹 스크래핑)
      - '우리가 접근할 수 있는 모든 정보는 웹에서 접근이 가능하다'라는 명제로 출발
      - 보안 데이터는 불가
      - 웹 사이트를 긁어서 원하는 데이터를 추출하여 전처리 적재하는 방식
      - request + beautifulsoup(bs4)
     
     2-4. level 4
      - crawling(크롤링)
      - 정보의 출처가 웹 사이트는 맞는데, 사람의 손을 타야지만 데이터를 획득할 수 있는 경우
      - ajax를 사용하거나, 디도스 방어가 들어가있는 등 사람 손을 거친 후에야만 접근 가능한 사이트가 대상
      - selenium(셀레니움) + 자동화(qt5 or 스케줄러를 활용)
    
    3) 데이터 준비
     - pandas(데이터처리, 분석), numpy(수학/과학용) 등을 사용
     - 데이터의 품질을 향상시킨다(여기에 주안점)
     3-1. 데이터 정제 : 결측치, 이상치 처리 
     3-2. 데이터 통합 : 여러 곳에서 가져온 데이터를 조합하여 데이터 구성
     3-3. 데이터 변환 : 데이터를 모델(4-5단계)에서 적합하게 사용되도록 변경 처리
    
    4) 데이터 탐색
     - pandas, matplotlib(시각화), seabon, d3.js
     - 데이터의 깊이를 이해하는데 중점 -> 통찰
     - 변수들의 상호 작용성, 시각적 분석법, 이상점 존재여부 체크
     - 통계적 분석, 시각적 분석, 단순 모델링 등을 사용
    
    5) 데이터 모델링 및 모델 구축
     - scikit-learn(머신러닝), tensorflow(딥러닝), keras(딥러닝)
     - 이전 단계로부터 획득한 모델, 도메인 지식, 데이터에 대한 통찰 등을 이용하여 연구 목표에 대한 답을 찾는 과정
       머신러닝, 딥러닝 : 예측모델을 구성, 정확도 높이고, 평가지수 고려 등 과정 진행
       -> 목표치에 도달하지 못하면 원점으로 돌아가서 다시 시작한다. (전면 재검토)
     - 필요하다면 통계학도 사용, 머신러닝, 딥러닝 등 운영과학기법을 총동원
     - 머신러닝 [학습, 정확도 평가, 파이프라인, 하이퍼파라미터 튜닝, 성능평가]

    6) 시스템 통합 혹은 레포트 발표
     - 1단계에 정한 결론에 대한 마무리
     - 레포트 형태, 보고서 형태, 시스템 형태, 솔루션의 형태 등 다양한 모습으로 결론이 도출된다
       시스템 형태 (웹 기반 : flask, DJango / GUI : gt5 / 백그라운드 서비스 : 순수 파이썬 + OS 종속적 구조)
 - 데이터 획득

4. 시각화의 카테고리
 - 시간 시각화
  > 막대(바)차트, 누적막대, 점그래프, 선그래프(연속데이터)
 - 분포 시각화
  > 파이차트(원), 도넛차트, 트리맵
 - 관계 시각화
  > 산점도, 산포도, 버블차트, 히스토그램, 네트워크 그래프(networkX)
 - 비교 시각화
  -> 히트맵, 체프론 페이스, 스타차트, 평행좌표계
 - 공간 시각화
  -> 지도, GIS