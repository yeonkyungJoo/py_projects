1. 아나콘다 가상환경 구축
 - Anaconda navigator 가동
 - Environments > Create > (name)DataCrawling, python 3.6 > 생성
 - cd ~/analysis/crawling/requirements.txt 생성 : 해당 프로젝트에서 사용하는 패키지를 기술
   ------------------------------------------------------------------------
   requests==2.18.4
   beautifulsoup4==4.6.0
   pandas==0.23.0
   numpy==1.14.3
   ------------------------------------------------------------------------
   Environments > DataCrawling > Open Terminal 
   (DataCrawling) $ cd C:\study\git\py_projects\analysis\crawling(프로젝트 내 requirements.txt 위치)
   - 가상환경 내에 설치된 패키지 목록 보기 (단, 약간의 목록 차이는 존재)
   (DataCrawling) $ conda list 
   or 
   (DataCrawling) $ pip list 
   - 프로젝트에 필요한 패키지 설치
   (DataCrawling) $ pip install -r requirements.txt
   or 
   (DataCrawling) $ conda install --file requirements.txt

2. 주피터 구동

3. 데이터 
 - 현재 상황
   1) 빅데이터
      원유, 전통적인 데이터 관리법으로 다루기 힘든 데이터를 다 아우른다
   
   2) 데이터 과학 => 새로운 가치(value) 생성
      정유공장 -> 대량의 데이터를 분석하여, 지식을 추출하는 방법까지 아우른다
   
   3) 구분
      > 빅데이터와 전통적인 데이터와 구분
      - 규모 : 얼마나 많은 데이터가 존재하고, 계속 생성되는가
      - 다양성 : 데이터의 종류가 얼마나 많은가
      - 속도 : 새로운 데이터가 얼마나 빠르게 생성되는가
      - 정확성 : 데이터가 얼마나 정확한가
   
   4) 통계학자, 데이터 과학자의 주된 차이점
      - 빅데이터를 다루는 능력, 하둡(스파크, 클러스터) 참고
      - 머신러닝(딥러닝) : AI>머신러닝>딥러닝
      - 컴퓨팅 능력
      - 알고리즘 구축 능력 (새로 만드는가? 혹은 조합을 만들어내는가?)
      
   5) 활용
      - 산업 : 고객, 업무, 직원, 상품에 대한 '통찰'을 얻기 위해 데이터 과학과 빅데이터를 활용 
      - 기업 : 교차판매, 상향 판매, 개인화 등등 -> 구글 애드샌드 활용 -> 맞춤광고 등
      - 예 
        a) 피플 애널리스트 : 텍스트 마이닝 적용 -> 직원 사기 점검, 직원간 네트워크 연구 등
            -> 머니볼(메이저리그 특정밑 이야기, 데이터 분석을 통해 발전시킨 사례)
        b) 금융기관 : 주식시장 예측, 대출 위험 평가, 신규 고객유치, 퀀트들의 개발 알고리즘을 통한 컴퓨터 자동 거래
        c) 정부기관 : 공공 데이터를 공개, 이를 통한 데이터 통찰을 수행할 수 있는 기회 제공, 어플의 개발 유도
            -> 수백만의 개인 감시용으로 사용 : 빅브라더 -> 이메일, 지도, 검색 등을 통해 수백만의 데이터를 수집하고 예측 
        d) 비정부기관 : NGO -> 자원봉사를 통해서 데이터를 구축 및 분석
        e) 대학 
            -> 연구, 학습 경험 개선, 코세라, 유닷시티, 에덱스 등
            -> 다양한 대학 기관들이 데이터를 제공, 
            -> 코렐, 머신러닝 때 따로 제공
 
 - 데이터의 종류
   1) 구조적 데이터
   2) 비구조적 데이터
   3) 자연어 데이터
   4) 기계 생성 데이터
   5) 그래프 기반 데이터(네트워크 데이터)
   6) 오디오, 비디오 데이터
   7) 스트리밍 데이터
      
 - 데이터과학 진행과정
    1) 연구목표 설정
     - 무엇을 조사?
     - 그 분석 결과로 회사/공공에서는 어떤 이익을 도출할 것인가?
     - 어떤 데이터와 자원이 필요한가? (일정, 업무분장 등)
     - 가장 중요한 것은 최종 결과물에 대한 검토 -> 시작점
     - 이런 결과물은 의사결정에 재료가 될 수도, 업무의 한 파트가 될 수도 있다
    
    2) 데이터 획득
     2-1. level 1
      - 제공이 된다
      - 사내 데이터, 공공 데이터, 대학 및 연구기관의 제공 데이터
      - 콘테스트 데이터(국내대회, 해외대회(kaggle))
      -> But, 상업성이 없고, 대부분 정제된 데이터 
     
     2-2. level 2
      - open API 사용
      - http 통신을 통해서 응답 데이터를 통해 수집
        ex) kakao, naver 등 포털이나 대기업이 제공하는 open API를 활용
      -> But, 쿼리 제한(일일 쿼리 수), 정제된 데이터
     
     2-3. level 3
      - web scraping(웹 스크래핑)
      - '우리가 접근할 수 있는 모든 정보는 웹에서 접근이 가능하다'라는 명제로 출발
      - 보안 데이터는 불가
      - 웹 사이트를 긁어서 원하는 데이터를 추출하여 전처리 적재하는 방식
      - request + beautifulsoup(bs4)
     
     2-4. level 4
      - crawling(크롤링)
      - 정보의 출처가 웹 사이트는 맞는데, 사람의 손을 타야지만 데이터를 획득할 수 있는 경우
      - ajax를 사용하거나, 디도스 방어가 들어가있는 등 사람 손을 거친 후에야만 접근 가능한 사이트가 대상
      - selenium(셀레니움) + 자동화(qt5 or 스케줄러를 활용)
    
    3) 데이터 준비
     - pandas(데이터처리, 분석), numpy(수학/과학용) 등을 사용
     - 데이터의 품질을 향상시킨다(여기에 주안점)
     3-1. 데이터 정제 : 결측치, 이상치 처리 
     3-2. 데이터 통합 : 여러 곳에서 가져온 데이터를 조합하여 데이터 구성
     3-3. 데이터 변환 : 데이터를 모델(4-5단계)에서 적합하게 사용되도록 변경 처리
    
    4) 데이터 탐색
     - pandas, matplotlib(시각화), seabon, d3.js
     - 데이터의 깊이를 이해하는데 중점 -> 통찰
     - 변수들의 상호 작용성, 시각적 분석법, 이상점 존재여부 체크
     - 통계적 분석, 시각적 분석, 단순 모델링 등을 사용
    
    5) 데이터 모델링 및 모델 구축
     - scikit-learn(머신러닝), tensorflow(딥러닝), keras(딥러닝)
     - 이전 단계로부터 획득한 모델, 도메인 지식, 데이터에 대한 통찰 등을 이용하여 연구 목표에 대한 답을 찾는 과정
       머신러닝, 딥러닝 : 예측모델을 구성, 정확도 높이고, 평가지수 고려 등 과정 진행
       -> 목표치에 도달하지 못하면 원점으로 돌아가서 다시 시작한다. (전면 재검토)
     - 필요하다면 통계학도 사용, 머신러닝, 딥러닝 등 운영과학기법을 총동원
     - 머신러닝 [학습, 정확도 평가, 파이프라인, 하이퍼파라미터 튜닝, 성능평가]

    6) 시스템 통합 혹은 레포트 발표
     - 1단계에 정한 결론에 대한 마무리
     - 레포트 형태, 보고서 형태, 시스템 형태, 솔루션의 형태 등 다양한 모습으로 결론이 도출된다
       시스템 형태 (웹 기반 : flask, DJango / GUI : gt5 / 백그라운드 서비스 : 순수 파이썬 + OS 종속적 구조)
 - 데이터 획득

