1. 아나콘다 가상환경 구축
 - Anaconda navigator 가동
 - Environments > Create > (name)DataCrawling, python 3.6 > 생성
 - cd ~/analysis/crawling/requirements.txt 생성 : 해당 프로젝트에서 사용하는 패키지를 기술
   ------------------------------------------------------------------------
   requests==2.18.4
   beautifulsoup4==4.6.0
   pandas==0.23.0
   numpy==1.14.3
   ------------------------------------------------------------------------
   Environments > DataCrawling > Open Terminal 
   (DataCrawling) $ cd C:\study\git\py_projects\analysis\crawling(프로젝트 내 requirements.txt 위치)
   - 가상환경 내에 설치된 패키지 목록 보기 (단, 약간의 목록 차이는 존재)
   (DataCrawling) $ conda list 
   or 
   (DataCrawling) $ pip list 
   - 프로젝트에 필요한 패키지 설치
   (DataCrawling) $ pip install -r requirements.txt
   or 
   (DataCrawling) $ conda install --file requirements.txt

2. 주피터 구동

3. 데이터 
 - 현재 상황
 - 데이터의 종류
 - 데이터과학 진행과정
    1) 연구목표 설정
     - 무엇을 조사?
     - 그 분석 결과로 회사/공공에서는 어떤 이익을 도출할 것인가?
     - 어떤 데이터와 자원이 필요한가? (일정, 업무분장 등)
     - 가장 중요한 것은 최종 결과물에 대한 검토 -> 시작점
     - 이런 결과물은 의사결정에 재료가 될 수도, 업무의 한 파트가 될 수도 있다
    
    2) 데이터 획득
     2-1. level 1
      - 제공이 된다
      - 사내 데이터, 공공 데이터, 대학 및 연구기관의 제공 데이터
      - 콘테스트 데이터(국내대회, 해외대회(kaggle))
      -> But, 상업성이 없고, 대부분 정제된 데이터 
     
     2-2. level 2
      - open API 사용
      - http 통신을 통해서 응답 데이터를 통해 수집
        ex) kakao, naver 등 포털이나 대기업이 제공하는 open API를 활용
      -> But, 쿼리 제한(일일 쿼리 수), 정제된 데이터
     
     2-3. level 3
      - web scraping(웹 스크래핑)
      - '우리가 접근할 수 있는 모든 정보는 웹에서 접근이 가능하다'라는 명제로 출발
      - 보안 데이터는 불가
      - 웹 사이트를 긁어서 원하는 데이터를 추출하여 전처리 적재하는 방식
      - request + beautifulsoup(bs4)
     
     2-4. level 4
      - crawling(크롤링)
      - 정보의 출처가 웹 사이트는 맞는데, 사람의 손을 타야지만 데이터를 획득할 수 있는 경우
      - ajax를 사용하거나, 디도스 방어가 들어가있는 등 사람 손을 거친 후에야만 접근 가능한 사이트가 대상
      - selenium(셀레니움) + 자동화(qt5 or 스케줄러를 활용)
    
    3) 데이터 준비
     - pandas(데이터처리, 분석), numpy(수학/과학용) 등을 사용
     - 데이터의 품질을 향상시킨다(여기에 주안점)
     3-1. 데이터 정제 : 결측치, 이상치 처리 
     3-2. 데이터 통합 : 여러 곳에서 가져온 데이터를 조합하여 데이터 구성
     3-3. 데이터 변환 : 데이터를 모델(4-5단계)에서 적합하게 사용되도록 변경 처리
    
    4) 데이터 탐색
     - pandas, matplotlib(시각화), seabon, d3.js
     - 데이터의 깊이를 이해하는데 중점 -> 통찰
     - 변수들의 상호 작용성, 시각적 분석법, 이상점 존재여부 체크
     - 통계적 분석, 시각적 분석, 단순 모델링 등을 사용
    
    5) 데이터 모델링 및 모델 구축
     - scikit-learn(머신러닝), tensorflow(딥러닝), keras(딥러닝)
     - 이전 단계로부터 획득한 모델, 도메인 지식, 데이터에 대한 통찰 등을 이용하여 연구 목표에 대한 답을 찾는 과정
       머신러닝, 딥러닝 : 예측모델을 구성, 정확도 높이고, 평가지수 고려 등 과정 진행
       -> 목표치에 도달하지 못하면 원점으로 돌아가서 다시 시작한다. (전면 재검토)
     - 필요하다면 통계학도 사용, 머신러닝, 딥러닝 등 운영과학기법을 총동원
     - 머신러닝 [학습, 정확도 평가, 파이프라인, 하이퍼파라미터 튜닝, 성능평가]

    6) 시스템 통합 혹은 레포트 발표
     - 1단계에 정한 결론에 대한 마무리
     - 레포트 형태, 보고서 형태, 시스템 형태, 솔루션의 형태 등 다양한 모습으로 결론이 도출된다
       시스템 형태 (웹 기반 : flask, DJango / GUI : gt5 / 백그라운드 서비스 : 순수 파이썬 + OS 종속적 구조)
 - 데이터 획득

